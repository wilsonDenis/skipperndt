import os
import sys


sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.donnees import charger_donnees, afficher_distribution
from src.modeles import creer_modele
from src.modeles.utilitaires import compter_parametres
from src.entrainement import Entraineur
from src.evaluation import (
    evaluer_modele,
    afficher_courbes_apprentissage,
    afficher_matrice_confusion,
    afficher_exemples_predictions,
    comparer_modeles,
)


def entrainer_un_modele(nom_modele, chargeur_train, chargeur_val, chargeur_test):
    """
    EntraÃ®ne un seul modÃ¨le et retourne ses rÃ©sultats.
    """
    print(f"\n{'='*60}")
    print(f"  MODÃˆLE : {nom_modele.upper()}")
    print(f"{'='*60}")

    
    modele = creer_modele(nom_modele)
    nombre_params = sum(p.numel() for p in modele.parameters())

   
    entraineur = Entraineur(modele, chargeur_train, chargeur_val)
    historique = entraineur.entrainer()

    
    afficher_courbes_apprentissage(historique)

    
    predictions, vraies_etiquettes, precision = evaluer_modele(modele, chargeur_test)
    afficher_matrice_confusion(predictions, vraies_etiquettes)

    return {
        "precision": precision,
        "parametres": nombre_params,
        "historique": historique,
        "modele": modele,
    }


def pipeline_comparatif():
    """
    EntraÃ®ne les 3 modÃ¨les (CNN Simple, CNN AmÃ©liorÃ©, U-Net)
    sur les mÃªmes donnÃ©es et les compare.
    """
    print("\n" + "=" * 30)
    print("  ENTRAÃNEMENT COMPARATIF DE TOUS LES MODÃˆLES")
    print("" * 30)


    print("\n Chargement des donnÃ©es...")
    (
        chargeur_train,
        chargeur_val,
        chargeur_test,
        dataset_train,
        dataset_test,
    ) = charger_donnees()

    if chargeur_train is None:
        print("Pas de donnÃ©es trouvÃ©es. ArrÃªt.")
        return

    afficher_distribution(dataset_train, dataset_test)

  
    modeles_a_tester = ["cnn_simple", "cnn_ameliore", "unet"]
    resultats = {}

    for nom in modeles_a_tester:
        resultats[nom] = entrainer_un_modele(
            nom, chargeur_train, chargeur_val, chargeur_test
        )

    print("\n" + "=" * 60)
    print("   COMPARAISON FINALE")
    print("=" * 60)

    for nom, res in resultats.items():
        print(f"  {nom:20s} | PrÃ©cision: {res['precision']*100:.2f}% | Params: {res['parametres']:>10,}")

        
    resultats_comparaison = {
        nom: {"precision": res["precision"], "parametres": res["parametres"]}
        for nom, res in resultats.items()
    }
    comparer_modeles(resultats_comparaison)

    
    meilleur_nom = max(resultats, key=lambda n: resultats[n]["precision"])
    print(f"\nğŸ† Meilleur modÃ¨le : {meilleur_nom} ({resultats[meilleur_nom]['precision']*100:.2f}%)")
    afficher_exemples_predictions(resultats[meilleur_nom]["modele"], chargeur_test, nombre=8)


if __name__ == "__main__":
    pipeline_comparatif()

